{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87cb858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dabbumothsera/Library/Python/3.9/lib/python/site-packages/google/api_core/_python_version_support.py:252: FutureWarning: You are using a Python version (3.9.6) past its end of life. Google will update google.api_core with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import fitz # PyMuPDF used for PDF processing\n",
    "from langchain_core.documents import Document # used to create Document objects\n",
    "from transformers import CLIPProcessor, CLIPModel # used for image and text embeddings\n",
    "from PIL import Image # used for image processing\n",
    "import torch # used for tensor operations\n",
    "import numpy as np # used for numerical operations\n",
    "from langchain.chat_models import init_chat_model #used for chat model interactions\n",
    "from langchain.prompts import PromptTemplate # used for prompt templates\n",
    "from langchain.schema.messages import HumanMessage # used for message formatting\n",
    "from sklearn.metrics.pairwise import cosine_similarity # used for similarity calculations\n",
    "import os # used for file operations\n",
    "import base64 # used for encoding images\n",
    "import io # used for in-memory file operations\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # used for text splitting\n",
    "from langchain_community.vectorstores import FAISS # used for vector store\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c90172d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##clip model\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "#set up the environment \n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "#initialize the model for unified embeddings\n",
    "clip_model=CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor=CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "313eb016",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embedding function for images and text\n",
    "def embed_image(image_data):\n",
    "    \"\"\"Embed an image using CLIP model.\"\"\"\n",
    "    if isinstance(image_data, str):# if image_data is a file path\n",
    "        image=Image.open(image_data).convert(\"RGB\") # Open image from file path\n",
    "    else: # if image_data is bytes\n",
    "        image=image_data\n",
    "    \n",
    "    input=clip_processor(images=image,return_tensors=\"pt\") # Process the image\n",
    "    with torch.no_grad():\n",
    "        features=clip_model.get_image_features(**input) # Get image features\n",
    "        #normalize the features to unit vector\n",
    "        features=features/ features.norm(dim=-1,keepdim=True)\n",
    "        return features.squeeze().numpy() # Return as numpy array\n",
    "    \n",
    "def embed_text(text):\n",
    "    \"\"\"Embed text using CLIP model.\"\"\"\n",
    "    input=clip_processor(text=[text],return_tensors=\"pt\",padding=True,truncation=True,max_length=77) # Process the text\n",
    "    with torch.no_grad():\n",
    "        features=clip_model.get_text_features(**input) # Get text features\n",
    "        #normalize the features to unit vector\n",
    "        features=features/ features.norm(dim=-1,keepdim=True)\n",
    "        return features.squeeze().numpy() # Return as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efbbc982",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PDF Processing and Document Creation\n",
    "pdf_path=\"multimodal_sample.pdf\" # Path to the PDF file\n",
    "doc=fitz.open(pdf_path) # Open the PDF file\n",
    "#storage for all documents\n",
    "documents=[] # List to store document texts\n",
    "all_embeddings=[] # List to store all embeddings\n",
    "image_data_store={} # Dictionary to store image data\n",
    "\n",
    "#text splitter\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50) # Initialize text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7d60408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document('multimodal_sample.pdf')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c59c1382",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,page in enumerate(doc):\n",
    "    ##process text\n",
    "    text=page.get_text() # Extract text from the page\n",
    "    if(text.strip()!=\"\"): # If text is not empty\n",
    "        #split text into chunks\n",
    "        text_chunks=splitter.split_text(text)\n",
    "        for chunk in text_chunks:\n",
    "            documents.append(Document(page_content=chunk,metadata={\"page\":i})) # Create Document object and add to list\n",
    "            emb=embed_text(chunk) # Embed the text chunk\n",
    "            all_embeddings.append(emb) # Add embedding to list\n",
    "\n",
    "## process images\n",
    "for img_index, img in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "            \n",
    "            # Create unique identifier\n",
    "            image_id = f\"page_{i}_img_{img_index}\"\n",
    "            \n",
    "            # Store image as base64 for later use with GPT-4V\n",
    "            buffered = io.BytesIO()\n",
    "            pil_image.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "            image_data_store[image_id] = img_base64\n",
    "            \n",
    "            # Embed image using CLIP\n",
    "            embedding = embed_image(pil_image)\n",
    "            all_embeddings.append(embedding)\n",
    "            \n",
    "            # Create document for image\n",
    "            image_doc = Document(\n",
    "                page_content=f\"[Image: {image_id}]\",\n",
    "                metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
    "            )\n",
    "            documents.append(image_doc)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_index} on page {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "198cec29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0}, page_content='Annual Revenue Overview\\nThis document summarizes the revenue trends across Q1, Q2, and Q3. As illustrated in the chart\\nbelow, revenue grew steadily with the highest growth recorded in Q3.\\nQ1 showed a moderate increase in revenue as new product lines were introduced. Q2 outperformed\\nQ1 due to marketing campaigns. Q3 had exponential growth due to global expansion.'),\n",
       " Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_0'}, page_content='[Image: page_0_img_0]')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fae59c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1649e2190>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create FAISS vector store\n",
    "embeddings_array=np.array(all_embeddings) # Convert embeddings to numpy array\n",
    "\n",
    "#create custom FAISS vector store since we have precomputed embeddings\n",
    "vector_store=FAISS.from_embeddings(\n",
    "    text_embeddings=[(doc.page_content,emb) for doc,emb in zip(documents,embeddings_array)],\n",
    "    embedding=None, # No embedding function since we have precomputed embeddings\n",
    "    metadatas=[doc.metadata for doc in documents] # Metadata for each document\n",
    ")\n",
    "vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2beff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello, world! It's great to connect with you. How can I help you today?\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--d0edef05-2d2e-438a-a790-64d881210576-0' usage_metadata={'input_tokens': 5, 'output_tokens': 50, 'total_tokens': 55, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 30}}\n"
     ]
    }
   ],
   "source": [
    "llm=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0) # Initialize Gemini Pro chat model\n",
    "resp=llm.invoke(\"Hello, world!\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee532ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_multimodal(query,top_k=3): #   Unified retrieval for text and images\n",
    "    \"\"\"Unified retrieval for text and images.\"\"\"\n",
    "    query_embedding=embed_text(query) # Embed the query text\n",
    "\n",
    "    results=vector_store.similarity_search_by_vector(\n",
    "        embedding=query_embedding,\n",
    "        k=top_k\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd4081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_message(query, retrieved_docs): # Create a multimodal message for the LLM\n",
    "    \"\"\"Create a message for the LLM including text and images.\"\"\"\n",
    "    message_content=query + \"\\n\\n\"\n",
    "    for doc in retrieved_docs:\n",
    "        if doc.metadata.get(\"type\")==\"image\":\n",
    "            image_id=doc.metadata[\"image_id\"]\n",
    "            image_base64=image_data_store[image_id]\n",
    "            # Include image in the message in a format GPT-4V understands\n",
    "            message_content+=f\"<image>{image_base64}</image>\\n\"\n",
    "        else:\n",
    "            message_content+=doc.page_content + \"\\n\\n\"\n",
    "    return HumanMessage(content=message_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98be434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_pdf_rag_pipeline(query):\n",
    "    \"\"\"Main pipeline for multimodal RAG.\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    context_docs = retrieve_multimodal(query, top_k=5)\n",
    "    \n",
    "    # Create multimodal message\n",
    "    message = create_multimodal_message(query, context_docs)\n",
    "    \n",
    "    # Get response from GPT-4V\n",
    "    response = llm.invoke([message])\n",
    "    \n",
    "    # Print retrieved context info\n",
    "    print(f\"\\nRetrieved {len(context_docs)} documents:\")\n",
    "    for doc in context_docs:\n",
    "        doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
    "        page = doc.metadata.get(\"page\", \"?\")\n",
    "        if doc_type == \"text\":\n",
    "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "            print(f\"  - Text from page {page}: {preview}\")\n",
    "        else:\n",
    "            print(f\"  - Image from page {page}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0fb9d194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What does the chart on page 1 show about revenue trends?\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 2 documents:\n",
      "  - Image from page 0\n",
      "  - Image from page 0\n",
      "\n",
      "\n",
      "Answer: The chart on page 1 shows a clear upward trend in revenue across the three quarters:\n",
      "\n",
      "*   **Q1 Revenue:** Approximately $250,000\n",
      "*   **Q2 Revenue:** Approximately $500,000\n",
      "*   **Q3 Revenue:** Approximately $950,000\n",
      "\n",
      "This indicates that revenue grew steadily from Q1 to Q2, and then experienced a much more significant, almost exponential, increase in Q3, making Q3 the quarter with the highest revenue.\n",
      "======================================================================\n",
      "\n",
      "Query: Summarize the main findings from the document\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 2 documents:\n",
      "  - Image from page 0\n",
      "  - Image from page 0\n",
      "\n",
      "\n",
      "Answer: The document summarizes the annual revenue trends across Q1, Q2, and Q3, showing a steady increase throughout the year, with the highest growth recorded in Q3.\n",
      "\n",
      "Key findings include:\n",
      "*   **Q1:** Experienced a moderate revenue increase due to the introduction of new product lines.\n",
      "*   **Q2:** Outperformed Q1, driven by successful marketing campaigns.\n",
      "*   **Q3:** Achieved exponential growth, primarily attributed to global expansion efforts.\n",
      "======================================================================\n",
      "\n",
      "Query: What visual elements are present in the document?\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieved 2 documents:\n",
      "  - Image from page 0\n",
      "  - Image from page 0\n",
      "\n",
      "\n",
      "Answer: Based on the provided document, the visual elements are:\n",
      "\n",
      "1.  **A main title or heading:** \"Annual Revenue Overview\"\n",
      "2.  **Blocks of text/paragraphs:** Providing an overview and details about the revenue trends.\n",
      "3.  **A chart:** The text explicitly states \"As illustrated in the chart below,\" indicating a visual representation of the revenue trends for Q1, Q2, and Q3.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example queries\n",
    "    queries = [\n",
    "        \"What does the chart on page 1 show about revenue trends?\",\n",
    "        \"Summarize the main findings from the document\",\n",
    "        \"What visual elements are present in the document?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "        answer = multimodal_pdf_rag_pipeline(query)\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
